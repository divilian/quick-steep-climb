
% TODO: appendix with Python

% TODO: "probing" a matrix with each of the basis vectors is enough to
% completely characterize the matrix.

\chapter{Matrices}

It's now time for the granddaddy of all linear algebra entities: the
\textbf{matrix}. When we've finished this part of our climb, you'll actually be
able to see the summit we'll eventually reach.

By the way, the plural of \textit{matrix} is \textbf{matrices} (pronounced
MAY-trih-sees), kind of like the plural of \textit{index} is \textit{indices.}
But don't forget the singular is still ``\textit{matrix}!'' Don't let me (or
anyone else) catch you uttering the non-word ``matrice'' -- you'll sound like a
dweeb and drive me up a wall.

\section{Row and column vectors}
\index{vector}

Up to now, a vector has simply been a vector. I haven't made a big deal about
how you write it on the page. We've been free to write a vector
$\overrightarrow{\textbf{x}}$ with the three elements 6, 2, and 9 in either of
these ways:

\vspace{-.15in}
\begin{center}
\begin{tabular}{ccc}
$\overrightarrow{\textbf{x}}$ = \textbf{[}$\ 6\ \ 2\ \ 9\ $\textbf{]} &
\quad\quad \textit{...or...} \quad\quad &
$\overrightarrow{\textbf{x}}$ = $\begin{bmatrix} 6 \\ 2 \\ 9 \end{bmatrix}$ \ 
\end{tabular}
\end{center}
\vspace{-.15in}

\index{function}

Or heck, you could even write it diagonally if you want. This flexibility is
because all that really matters is the \textit{function} view of a vector that
we discussed in section~\ref{vectorIsFunction}. All that ultimately matters is
that you associate the correct index number with the correct element. However I
might draw $\overrightarrow{\textbf{x}}$ on paper, if I asked you for the value
of ``element \#0,'' you'd say 6, and if I asked for ``element \#2,'' you'd say
9. The way it looks has been immaterial up until now.

\index{row vector}
\index{column vector}

That will still be true sometimes. But beginning with this chapter, it's going
to sometimes turn out to matter whether or not we think of a vector as a
\textbf{row vector} (the left-hand-side version of
$\overrightarrow{\textbf{x}}$, above) or a \textbf{column vector} (the
right-hand-side). Memorize these terms: they matter, and you'll have to have
them on the tip of your neural cortex. A row goes horizontally, side-to-side;
and a column goes vertically, up-to-down.

I'll try to always be very careful to emphasize the row vs.~column nature of a
vector in those cases where it turns out to matter.

\index{default}

By the way, one surprising thing (at least, it was to me) is that the
``default'' is for an unspecified vector to be treated as a \textit{column}
vector, not a row. Column vectors take up more room on the page, and aren't as
natural when you're writing on paper, which I guess is why it surprised me. At
any rate, whenever a vector is under discussion, try to visualize it as an
up-and-down column of entries, unless the accompanying text explicitly says
otherwise.

\section{The matrix}

\index{matrix (plural: matrices)}

At last, the matrix. This will seem underwhelming at first, but \textit{boy}
does it pack a wallop.

A matrix is simply a two-dimensional rectangular grid of entries, kind of like
a spreadsheet. We'll use capital letters to designate them, with no special
arrow-like or other adornment. Here's our first example:

\vspace{-.15in}
\begin{align*}
A =
\begin{bmatrix}
5 &-7 &3 &9 \\
18 &4 &1 &1 \\
3 &-3 &\pi &4 \\
\end{bmatrix}
\end{align*}
\vspace{-.15in}

\index{dimension}

Matrices are always rectangular, but not always square. The $A$ matrix is
called a ``$3\times 4$'' (three-by-four) matrix, since it has three
\textbf{row}s and four \textbf{column}s. We say that $3\times 4$ are the
matrix's \textbf{dimensions}. Again, it's important to master all this
terminology. When giving the dimensions of a matrix, you always list the number
of rows first, and then the number of columns.

\index{index number}

To specify an individual element, we need \textit{two} indices instead of just
one as we did for a vector. We'll use Python-style numbering (starting with 0)
and write the row and column as a two-part comma-separated subscript:

\vspace{-.15in}
\begin{align*}
A_{0,0} &= 5 \\
A_{1,0} &= 18 \\
A_{0,3} &= 9 \\
A_{2,2} &= \pi \\
\end{align*}
\vspace{-.25in}

Just practice first moving down to the correct row, then moving over to the
correct column, and you'll be fine.

\subsection{Labels}

\index{label}

As with vectors, we won't always use index numbers to designate rows and
columns: sometimes we'll use labels. Check out this matrix $W$ (for
``weather''):

\index{Fredericksburg, Virginia}
\index{Washington, D.C.}
\index{Richmond, Virginia}

\vspace{-.4in} 
\begin{adjustwidth}{}{60pt}
\begin{center}
\begin{multicols}{2}
\begin{flushright}
\hspace*{1cm} \\
\hspace*{1cm} \\
\footnotesize{D.C.} \\
\footnotesize{Fredericksburg} \\
\footnotesize{Richmond} \\
\end{flushright}
\columnbreak
\vspace{-1.5in} 
\begin{align*}
\begin{bmatrix}
81 \ & \ 86 \ & \ 78 \ & \ 74 \ & \ 77 \\
83 \ & \ 86 \ & \ 79 \ & \ 79 \ & \ 82 \\
82 \ & \ 86 \ & \ 84 \ & \ 87 \ & \ 87 \\
\end{bmatrix}
\end{align*}
\vspace{-.15in}
\scriptsize{Mon} \ \  \scriptsize{Tue} \ \ \scriptsize{Wed} \ \ \scriptsize{Thu} \ \ \scriptsize{Fri} \\
\end{multicols}
\end{center}
\end{adjustwidth}
\vspace{-.15in}

Here we're using city names for the row labels, and days of the week as the
column labels. It's still easy peasy to interpret -- how hot did it get in the
nation's capital on Tuesday? 86\textdegree, of course. Using the same subscript
notation as above, we could say:

\vspace{-.15in}
\begin{align*}
W_{\textrm{D.C.},\textrm{Mon}} &= 81 \\
W_{\textrm{Fredericksburg},\textrm{Wed}} &= 79 \\
W_{\textrm{Fredericksburg},\textrm{Thu}} &= 79 \\
W_{\textrm{Richmond},\textrm{Thu}} &= 87 \\
&\vdots \\
\end{align*}

and so forth. D.C. and Fred had a bit of a cool-down midweek, thank God, while
Richmond was all the while cooking in the upper 80's.

\section{A matrix is also a function}
\label{matrixIsFunction}
\index{function}
Remember back in section~\ref{vectorIsFunction} (p.~\pageref{vectorIsFunction})
when I explained that a vector, viewed in a sufficiently weird way, was
actually a function? The same thing is true for matrices, just by adding one
more input to the function.

Put another way, let's consider the row labels (or numbers, if we want to be
boring) as the set $C$ (for ``cities''). And let's consider the column labels
as the set $D$ (for ``days-of-the-week''). Then, you can see that a matrix is
precisely maps a pair of a city and a day to a high temperature. (The high
temperatures are in the set $\mathbb{R}$, which are the real numbers.) In
symbols, $W$ is defined as this function:

\vspace{-.15in}
\begin{align*}
W : C \times D \rightarrow \mathbb{R}
\end{align*}
\vspace{-.15in}

\index{domain}
\index{codomain}
\index{Cartesian product}
Recall that function syntax. $W$ is the name of the function. The part before
the arrow is the \textbf{domain} of the function: the set which its inputs are
drawn from. Since it's the Cartesian product of two sets (cities and days) this
domain is really all the ordered pairs of cities-and-days, like (D.C., Thurs)
and (Richmond, Monday). The function takes any ordered pair like that and gives
you a number telling you how hot that city was on that day. It's a snap when
seen this way.


\section{Matrix operations}

Just as section~\ref{vectorOps} listed the permissible actions we could perform
on vectors (and scalars), so this section lists the operations we can perform
on matrices (and vectors, and scalars). There's one other big one which I'll
save for entire separate chapter, but there are still four useful ones we'll
cover here.

\subsection{Operation \#1: scalar-matrix multiplication}
\label{scalarMatrixMultiplication}

This one's a piece of cake. Recall that multiplying a scalar by a vector
amounted to multiplying the scalar by each of its elements, producing a vector
of the same dimension. Same here: we get a matrix of the same dimension by
multiplying individually:

\vspace{-.15in}
\begin{align*}
4 \cdot
\begin{bmatrix}
3 & 2 & 9 \\
1 & -1 & 0 \\
\end{bmatrix}
=
\begin{bmatrix}
12 & 8 & 36 \\
4 & -4 & 0 \\
\end{bmatrix}.
\end{align*}
\vspace{-.15in}

Sometimes we'll put a dot between the two, as above, though we'll often omit
that and just write the scalar and matrix side-by-side. Either way, it means
scalar-matrix multiplication.

\subsection{Operation \#2: matrix addition}

Also a piece of cake, and just what you'd expect:

\vspace{-.15in}
\begin{align*}
\begin{bmatrix}
4 & 1 \\
1 & -2 \\
3 & 18 \\
\end{bmatrix}
\begin{bmatrix}
1 & 2 \\
5 & 2 \\
-10 & -10 \\
\end{bmatrix}
+
\begin{bmatrix}
5 & 3 \\
6 & 0 \\
-7 & 8 \\
\end{bmatrix}.
\end{align*}
\vspace{-.15in}

The only hard part is not going cross-eyed as you zigzag your eyeballs across
the page to match up entries.

As with vector addition, you simply can't add two matrices at all if they don't
have the same dimensions. Also just like vectors, we can \textit{subtract} one
matrix from another just by adding the first matrix to ``$-1$ times the second
matrix.''

\subsection*{Operation \#3: transpose}

\index{transpose}

Now here's kind of a strange one that's more structural than arithmetical. It's
called the \textbf{transpose} operator. Unlike the previous ones, this is a
\textbf{unary operator} which means it acts on only one operand (input) instead
of two. (When we did scalar-matrix multiplication, we needed two things to act
on: a scalar and a matrix. With matrix addition, we needed two matrices. But
here, we only need one ``thing'' that we do the transpose to.)

The symbol for this is a superscript ``$\intercal$'' written just after the
matrix. Its purpose is to \textit{interchange the rows and the columns.} The
rows of the original matrix become the columns (in the same order) of the
transposed matrix, and vice versa. So,

\vspace{-.15in}
\begin{align*}
\textrm{If } A = 
\begin{bmatrix}
4 & 1 \\
1 & -2 \\
3 & 18 \\
\end{bmatrix},
\ \textrm{then } A^\intercal = 
\begin{bmatrix}
4 & 1 & 3 \\
1 & -2 & 18 \\
\end{bmatrix}.
\end{align*}

As you can see, if we start with an $m\times n$ matrix, transposing it gives us
an $n\times m$ matrix. Fat-and-wide becomes tall-and-skinny, and vice versa.
And in symbols,

\vspace{-.25in}
\begin{align*}
A_{i,j} = A^\intercal_{j,i}
\end{align*}
\vspace{-.25in}

for all the rows $i$ and columns $j$ of the $A$ matrix. Note how the $i$ and
$j$ swapped places in the subscript. The element at row 7, column 19 of $A$ is
the same as the one at row 19, column 7 of $A^\intercal$.

\smallskip

\index{row vector}
\index{column vector}
\label{rowAndColVectors}

You can probably also tell that if we transpose a column vector, we get a row
vector, and vice versa. As I mentioned, there are times that we'll treat a
vector as just a sequence of elements, and won't care about its ``shape.''
Other times, though (including the next operation) we'll really be treating it
as a sort of degenerate matrix with only one row or one column. At those times,
it makes sense to say things like:

\vspace{-.15in}
\begin{align*}
\textrm{If } \overrightarrow{\textbf{x}} = 
\begin{bmatrix}
9 \\
2 \\
4 \\
\end{bmatrix},
\ \textrm{then } \overrightarrow{\textbf{x}}^\intercal = 
\begin{bmatrix}
9 & 2 & 4 \\
\end{bmatrix}.
\end{align*}

When speaking, by the way, you pronounce
$\overrightarrow{\textbf{x}}^\intercal$ as ``x-transpose'' and $A^\intercal$ as
``A-transpose.''

\subsection{Operation \#4: matrix-vector multiplication}

\label{matrixVectorMultiplication}

Okay, heads-up. Here's the toughie.

What do you think it would mean to multiply a \textit{matrix} by a
\textit{vector}? You might think of several plausible ways to define such an
operation, but I doubt you'll think of the one that's actually in use. It's
weird for a number of reasons, one of which is that the thing you get back
often isn't the same dimensions as \textit{either} of the operands!

Let me just do one and see if you can reverse engineer how I got the answer.
Here goes:

\vspace{-.15in}
\begin{align*}
\begin{bmatrix}
2 & 2 & 7 \\
1 & 4 & 0 \\
\end{bmatrix} \cdot
\begin{bmatrix}
1 \\ -1 \\ 4 
\end{bmatrix} = 
\begin{bmatrix}
28 \\ -3
\end{bmatrix}.
\end{align*}

Can you figure out why in the world that would be the answer?

Surprise number one is the \textit{type} of thing we got back. We multiplied a
$2\times 3$ matrix by a three-dimensional column vector and this produced...a
\textit{two}-dimensional column vector. Ooookay. Surprise number two are the
contents of that vector. 28 and $-3$, what the...?

Things will get considerably clearer if you remember the \textit{dot product}
operator from section~\ref{dotProduct} (p.~\pageref{dotProduct}). What we did
here was \textit{separately compute \underline{each} row of the matrix
dot-product-ed with the vector}. A dot product, you'll recall, gives us a
single scalar as an answer. So, since $[\ 2\ \ 2\ \ 7\ ] \cdot [\ 1\ \ -1\ \ 4\
]$ is 28, and $[\ 1\ \ 4\ \ 0\ ] \cdot [\ 1\ \ -1\ \ 4\ ]$ is $-3$, our result
is a vector containing those two results.

This can really trip you up later if you don't master it now, so take a minute
and master the above calculation. Make sure you understand how the
\textit{rows} of the matrix are each dot-product-ed with the \textit{column}
vector to produce an answer. And since there are two such dot products, there
are two such answers, and the result is a column vector with two entries.

The rules for when matrix-vector multiplication are possible are:

\begin{compactenum}
\label{matVecRules}
\item The vector must be a column vector, or at least treated as such for
purposes of performing the operation.
\item The number of \textit{columns} (not rows) in the matrix must be the same
as the dimension of (the number of entries in) the vector. If these do not
match, the game is over.
\end{compactenum}

If these two check out, then the operation is permissible, and the result is a
column vector whose dimension (number of entries) is the same as the number of
\textit{rows} (not columns) in the matrix.

For example:

\begin{compactitem}
\item A wide $5\times 17$ matrix times a 17-dimensional column vector is legal,
and gives a 5-dimensional vector result.
\item A tall $17\times 5$ matrix times a 5-dimensional column vector is legal,
and gives an 17-dimensional vector result.
\item A wide $5\times 17$ matrix times a 5-dimensional column vector is
illegal.
\item A wide $5\times 17$ matrix times any \textit{row} vector is illegal.
\end{compactitem}

\medskip

Sometimes it helps to stretch your neck out a bit before attempting
matrix-vector multiplication. That's because you have to visualize a
(potentially long) row of numbers being paired up, one-by-one, with a
(potentially tall) column of numbers. It's not too hard once you get used to
it, but it can be surprisingly difficult at first to move across the page with
your left eyeball at the same time you're moving \textit{down} the page with
your right eyeball. Like all things, practice.


\subsection{Two ways to think about matrix-vector multiplication}

Now there are two different ways to think about the matrix-vector product. Each
one is useful in certain situations, so it's very important to master
\textit{both} interpretations. Here they are:

\begin{framed}
\label{twoInterpretations}
\begin{samepage}
Two different ways to think about $A \cdot \overrightarrow{\textbf{x}}$:
\begin{compactenum}
\item All of the dot products between the \textit{rows} of $A$ with
$\overrightarrow{\textbf{x}}$.
\item A linear combination of $A$'s \textit{columns}.
($\overrightarrow{\textbf{x}}$'s elements are the coefficients of the
linear combination.)
\end{compactenum}
\end{samepage}
\end{framed}

The first interpretation is what we've learned operationally so far. To compute
$A \cdot \overrightarrow{\textbf{x}}$, we take each of $A$'s rows, in turn, and
dot-product them with the vector, producing a new vector of answers.

\index{Jezebel}
\index{matchmaker@\texttt{matchmaker.com}}

When does it make sense to think of it this way? Think back to Jezebel and
friends (p.~\pageref{matchmakerExample}). Let's say we have Jezebel's
(normalized) survey answers in a vector $\overrightarrow{\textbf{j}}$:

\begin{center}
\begin{tabular}{cccccc}
$\overrightarrow{\textbf{j}}$ = [ & .434 & .173 & .867 & .173 & ]. \\
& \scriptsize{action} & \scriptsize{hiking} & \scriptsize{candle} &
\scriptsize{mystery} & \\
\normalsize
\end{tabular}
\end{center}
\vspace{-.15in}

Now suppose we have all the eligible Men's (normalized) survey answers in a
matrix $M$, like so:

\vspace{-.3in} 
\begin{adjustwidth}{}{30pt}
\begin{center}
\begin{multicols}{2}
\begin{flushright}
\hspace*{1cm} \\
\hspace*{1cm} \\
\footnotesize{biff} \\
\footnotesize{filbert} \\
\footnotesize{wendell} \\
%\footnotesize{mrright} \\
\end{flushright}
\columnbreak
\vspace{-1.5in} 
\begin{align*}
\begin{bmatrix}
.704 \ & \ .704 \ & \ .070 \ & \ .070 \\
.548 \ & \ .183 \ & \ .730 \ & \ .365 \\
.092 \ & \ .275 \ & \ .275 \ & \ .917 \\
%.500 \ & \ .500 \ & \ .500 \ & \ .500 \\
\end{bmatrix}
\end{align*}
\vspace{-.15in}
\hspace*{-.1in}
\ \scriptsize{action} \ \ \ \scriptsize{hiking} \ \  \scriptsize{candle} \ \ 
\scriptsize{mystery} \\
\end{multicols}
\end{center}
\end{adjustwidth}
\vspace{-.15in}

A very reasonable way to store these entries, I think you'll agree. Each row
represent one man's (normalized) survey responses, where each column is one of
the questions.

\index{transpose}

Now we can compute Jezebel's compatibility with every guy under the sun in one
fell swoop! All we need to do is transpose her vector into a column vector, and
perform matrix-vector multiplication:

\vspace{-.15in}
\begin{align*}
M \cdot \overrightarrow{\textbf{j}}^\intercal =
\begin{bmatrix}
.704 \ & \ .704 \ & \ .070 \ & \ .070 \\
.548 \ & \ .183 \ & \ .730 \ & \ .365 \\
.092 \ & \ .275 \ & \ .275 \ & \ .917 \\
%.500 \ & \ .500 \ & \ .500 \ & \ .500 \\
\end{bmatrix} \cdot 
\begin{bmatrix}
.434 \\ .173 \\ .867 \\ .173 \\
\end{bmatrix} =
\begin{bmatrix}
.500 \\ .966 \\ .484 \\ % .824 \\
\end{bmatrix}.
\end{align*}
\vspace{-.15in}

Boom! The elements of our answer represent Jezebel's compatibility with Biff,
Filbert, and Wendell, respectively. That's because the \textit{first} element
of our answer was the dot product of the \textit{first} row of $M$ (Biff's row)
with $\overrightarrow{\textbf{j}}$. Ditto for the other two.

As you can see, this is really nothing more than automating and repeating our
individual dot product calculations, since that's what the matrix-vector
product \textit{is}, according to interpretation 1. Instead of just multiplying
Jezebel's vector times Biff's, we can multiply it times a thousand different
guys all at once, to get a thousand different compatibilities. In the end, we
see that Filbert and Jezebel are paired together as expected, which means all
is right with the world.

\bigskip

\index{bake sale}
\index{brownies}
\index{chocolate chip cookies}
\index{fudge}
\index{Rice Krispie treats}

\label{linearComboOfColumns}

The other way to think about the matrix-vector product is as a linear
combination of the matrix's \textit{columns}. The bake sale example from
p.~\pageref{bakeSale} is a case where it makes sense to think about it this
way. Let's make a ``Recipes matrix'' $R$ in which each column is one recipe,
and each row corresponds to an ingredient. The entries tell us the quantity of
that ingredient required for one batch of each of the recipes:

\vspace{-.3in} 
%\begin{adjustwidth}{}{0pt}
\begin{center}
\begin{multicols}{2}
\begin{flushright}
\hspace*{1cm} \\
\hspace*{1cm} \\
\small{butter} \\
\small{sugar} \\
\small{chips} \\
\small{flour} \\
\small{eggs} \\
\small{marshmallow} \\
\small{rice krispies} \\
\end{flushright}
\columnbreak
\vspace{-1.5in} 
\begin{align*}
\setlength\arraycolsep{13pt}
\begin{bmatrix}
2 \ & \ 1 \ & \ 2 \ & \ 2 \\
1 \ & \ 1 \ & \ 2 \ & \ 0 \\
1 \ & \ 4 \ & \ 4 \ & \ 0 \\
1 \ & \ 2 \ & \ 0 \ & \ 0 \\
3 \ & \ 2 \ & \ 0 \ & \ 0 \\
0 \ & \ 0 \ & \ 0 \ & \ 5 \\
0 \ & \ 0 \ & \ 0 \ & \ 2 \\
\end{bmatrix}
\end{align*}
\vspace{-.15in}
\hspace*{-.1in}
\scriptsize{chippers} \ \ \scriptsize{brownies} \ \  \scriptsize{fudge} \ 
\scriptsize{rice krispies} \\
\end{multicols}
\end{center}
%\end{adjustwidth}
\vspace{-.15in}

When we do our bake sale planning, we decide we want to bake the following
number of \textbf{b}atches for our first day of the sale:

\begin{center}
\begin{tabular}{cccccc}
$\overrightarrow{\textbf{b}}$ = [ & 5 & 5 & 2 & 6 & ]. \\
& \scriptsize{chippers} & \scriptsize{brownies} & \scriptsize{fudge} &
\scriptsize{rice krispies} & \\
\normalsize
\end{tabular}
\end{center}
\vspace{-.15in}

How do we compute our complete shopping list? Simple: transpose
$\overrightarrow{\textbf{b}}$ and do matrix-vector multiplication:

\vspace{-.15in}
\begin{align*}
R \cdot \overrightarrow{\textbf{b}}^\intercal =
\begin{bmatrix}
2 \ & \ 1 \ & \ 2 \ & \ 2 \\
1 \ & \ 1 \ & \ 2 \ & \ 0 \\
1 \ & \ 4 \ & \ 4 \ & \ 0 \\
1 \ & \ 2 \ & \ 0 \ & \ 0 \\
3 \ & \ 2 \ & \ 0 \ & \ 0 \\
0 \ & \ 0 \ & \ 0 \ & \ 5 \\
0 \ & \ 0 \ & \ 0 \ & \ 2 \\
\end{bmatrix} \cdot 
\begin{bmatrix}
5 \\ 5 \\ 2 \\ 6 \\
\end{bmatrix} =
\begin{bmatrix}
31\\ 14\\ 33\\ 15\\ 25\\ 30\\ 12 \\
\end{bmatrix}.
\end{align*}
\vspace{-.15in}

We'll clean out Publix by purchasing 31 sticks of butter, 14 cups of sugar, 33
bags of chocolate chips, and so on.

Now why do I say that the \textit{second} interpretation from
p.~\pageref{twoInterpretations} is the right one here? Because we're treating
the columns as meaningful entities. The left-most column is ``how much stuff to
buy for \textit{each} chocolate chip cookie batch.'' The second column is ``how
much stuff to buy for each brownie batch.'' And so forth. So what we're really
doing in this calculation is saying ``we want 5 chipper recipes, so 5 times the
first column; and also 5 brownie recipes, so 5 times the second column; 2
batches of fudge, so twice the third column; plus 6 times the last column for
our 6 trays of Rice Krispie treats.'' Mathematically, we're doing this:

\vspace{-.15in}
\begin{align*}
5 
\begin{bmatrix}
2  \\
1  \\
1  \\
1  \\
3  \\
0  \\
0  \\
\end{bmatrix} +
5 
\begin{bmatrix}
1\\
1\\
4\\
2\\
2\\
0\\
0\\
\end{bmatrix} +
2 
\begin{bmatrix}
2 \\
2 \\
4 \\
0 \\
0 \\
0 \\
0 \\
\end{bmatrix} +
6 
\begin{bmatrix}
2 \\
0 \\
0 \\
0 \\
0 \\
5 \\
2 \\
\end{bmatrix} =
\begin{bmatrix}
31\\ 14\\ 33\\ 15\\ 25\\ 30\\ 12 \\
\end{bmatrix},
\end{align*}
\vspace{-.15in}

which is exactly a linear combination of the recipes. The coefficient of each
vector -- the scalar in each scalar-vector multiplication -- is the
corresponding element of our $\overrightarrow{\textbf{b}}$ vector, telling us
how much of each recipe to make.

\bigskip

A couple additional thoughts before we leave this section. First, I don't know
about you, but I think it's actually quite remarkable that these two different
definitions of matrix-vector multiplication turn out to give the same answer.
In \#1, we're taking the dot product of each of the matrix's rows with the
entire vector. In \#2, we're working with the columns of the matrix, not the
rows, and we're not doing any dot products at all; we're treating the vector as
a sequence of coefficients and doing scalar-vector multiplication with them.
These seem like totally different -- perhaps even opposite, or competing --
operations, yet they always work out to the same result.

Finally, I'll just say that if you think it's annoying to take the transpose of
the vector and write it up-and-down, I feel your pain. It does seem much more
sensible (and less prone to error) to just leave the row vector as a row
vector, because then it lines up visually with the \textit{rows} of the matrix
that we're taking its dot product with anyway. It's hard enough to keep all the
numbers straight without having to constantly shift between horizontal and
vertical vision \textit{as} you're calculating! Anyway, I guess I've just
learned to accept this over the years, since it's universal to define the
operation this way. My advice is to grumble to yourself a while and then try to
do the same.

\section{``Special'' matrices}

Finally, there are some terms for ``special'' matrices that satisfy certain
properties, which will come up for us in important contexts. We'll start with
the least restrictive definition and repeatedly add further constraints to it
for successively more restrictive ones.

\index{square matrix}
Easiest of all, a \textbf{square} matrix is simply one that has the same number
of rows and columns. Any $4\times 4$ matrix is square, and no $6\times 7$
matrix is square. Simple. And yes, in case you're wondering, we can have a
$1\times 1$ matrix, which is considered square.

\pagebreak

\index{symmetric matrix}

In order for a matrix to be \textbf{symmetric}, it has to be
square.\footnote{There are more advanced versions of some of the definitions in
this section that don't require square-ness, but they won't come up for us.}
Further, in a symmetric matrix \textit{the rows and the columns are
interchangeable.} For example:

\vspace{-.15in}
\begin{align*}
\begin{bmatrix}
9 & 7 & 2 & 8\\
7 & -3 & 0 & -1\\
2 & 0 & 5 & 4 \\
8 & -1 & 4 & 16 \\
\end{bmatrix}
\end{align*}
\vspace{-.15in}

In this matrix, row \#0 is $[\ 9\ \ 7\ \ 2\ \ 8\ ]$, and column \#0 is
\textit{also} $[\ 9\ \ 7\ \ 2\ \ 8\ ]$. Similarly, row \#1 and column \#1 are
the same, as are row \#2 and column \#2, and row \#3 and column \#3.

\label{symmetric}

Mathematically, a matrix $M$ is symmetric only if:

\vspace{-.15in}
\begin{align*}
\forall i,j \ M_{i,j} = M_{j,i}.
\end{align*}
\vspace{-.15in}

In other words, you must be able to swap the row and column numbers and get the
same value. The entry at row 5 column 8 must be the same as the one at row 8
column 5, \textit{etc.} If that's not true every time, it's not a symmetric
matrix.

It might help you understand the meaning of ``symmetric'' if you mentally
visualize a line from the upper-left entry to the lower-right entry of the
matrix:

\vspace{-.15in}
\begin{align*}
\begin{bmatrix} \tikzmark{a}
9 & 7 & 2 & 8\\
7 & -3 & 0 & -1\\
2 & 0 & 5 & 4 \\
8 & -1 & 4 & 16 \tikzmark{b} \\
\end{bmatrix}
\end{align*}
\MyLine[thick]{a}{b}
\vspace{-.15in}

\index{main diagonal, of a matrix}
\index{diagonal, of a matrix}
\index{mirror}

These entries, by the way, where the row number equals the column number, are
called the \textbf{main diagonal} (or just the \textbf{diagonal}) of the
matrix. If a matrix is symmetric, you could imagine that line being a mirror:
and it perfectly reflects one side to the other. The two 7's are on matching,
opposite sides of that line, as are the two 2's, the two 0's, the two 8's, the
two $-1$'s, and the two 4's. Note that the entries \textit{on} the main
diagonal can be anything at all -- they don't affect the symmetric-ness (or
not) of the matrix.

This property may seem obscure, but it will come up surprisingly often.

By the way, you might have expected a ``symmetric matrix'' to be one that was a
mirror image of itself left-to-right-wise, or top-to-bottom-wise. Nope. That
doesn't turn out to be a useful concept. The definition above, however --
reflection along the main diagonal -- turns out to be immensely useful.

\medskip

\index{upper-triangular}
Okay, next one. A matrix is said to be \textbf{upper-triangular} if \textit{all
entries below the main diagonal are zero.} Example:

\vspace{-.15in}
\begin{align*}
\begin{bmatrix}
9 & 7 & 2 & 8\\
0 & -3 & 0 & -1\\
0 & 0 & 5 & 4 \\
0 & 0 & 0 & 16 \\
\end{bmatrix}
\end{align*}
\vspace{-.15in}

This matrix is upper-triangular because I've zeroed-out everything below the
$9,-3,5,16$ entries of the main diagonal. Note that it's perfectly fine to also
have a 0 above -- or even on -- the diagonal. The existence of such a zero
doesn't disqualify you from upper-triangular-ness. The only requirement is that
all entries \textit{below} the diagonal must be zero.

\index{lower-triangular}

You can probably tell why the word ``triangular'' is used: the possibly-nonzero
entries are all arranged in a right triangle in the upper-right half of the
matrix. Occasionally it will also be useful to talk about a
\textbf{lower-triangular} matrix, which has zeroes in every entry
\textit{above} the diagonal. And again, you have to be square for your upper-
or lower- triangular-ness to even be under consideration.

\medskip

\index{diagonal matrix}

Also common is the notion of a \textbf{diagonal matrix}. Be careful: a ``a
diagonal matrix'' is different from ``the diagonal \textit{of} a matrix!'' A
diagonal matrix is one where all entries \textit{not} on the diagonal must be
zero. Example:

\vspace{-.15in}
\begin{align*}
\begin{bmatrix}
9 & 0 & 0 & 0\\
0 & -3 & 0 & 0\\
0 & 0 & 5 & 0 \\
0 & 0 & 0 & 16 \\
\end{bmatrix}
\end{align*}
\vspace{-.15in}

Food for thought: a diagonal matrix is both upper-triangular \textit{and}
lower-triangular.

Like I said, we're getting more and more restrictive as we go. If you're
planning on being a $6\times 6$ diagonal matrix, there's a whole lot of your
life already set in stone. The only choices you have are what to put on your
diagonal.

\medskip

\index{identity matrix}
\label{identityMatrix}

But we can get more restrictive still! Believe it or not, a very common and
interesting type of matrix will be a so-called \textbf{identity matrix}. Get
this: an identity matrix is a diagonal matrix \textit{with only 1's on the
diagonal.} Talk about confining. Once you've decided on your size, you have
literally no choices. The one and only $4\times 4$ identity matrix is this one:

\vspace{-.15in}
\begin{align*}
\begin{bmatrix}
1 & 0 & 0 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
\end{bmatrix}
\end{align*}
\vspace{-.15in}

For every natural number $n$, there's one and only one identity matrix of that
size. Sometimes we call that matrix $I_n$ for short; in other words, the matrix
above is sometimes called $I_4$. 

In the next chapter (p.~\pageref{identityMatrixExplanation}, to be exact),
you'll learn the excellent reason that this kind of matrix is called an
``identity'' matrix. For now, test yourself with these questions:

\begin{compactenum}
\label{specialMatrixQuiz}
\item Are all square matrices symmetric?
\item Are all symmetric matrices square?
\item Are all symmetric matrices upper-triangular?
\item Are all upper-triangular matrices symmetric?
\item Are all diagonal matrices upper-triangular?
\item Are all upper-triangular matrices diagonal?
\item Are all diagonal matrices symmetric?
\item Are all diagonal matrices identity matrices?
\item Are all identity matrices upper-triangular?
\item Are all identity matrices diagonal?
\end{compactenum}

The answers are at the end of the chapter (p.~\pageref{specialMatrixQuizSols}).

\bigskip

\index{block diagonal matrix}
Finally, one more special matrix type that's a bit different from the previous
ones. It's called a \textbf{block diagonal matrix}. A matrix is block diagonal
if it can be partitioned into rectangular chunks (called ``blocks'') such that
each chunk on the diagonal is square, and all entries in the non-diagonal
chunks must be all zeros.

Whoa, that's hard to visualize. Let's look at some examples:

\vspace{-.15in}
\begin{align*}
\mleft[
\begin{array}{cccc}
3 & 2 & 0 & 0\\
4 & 4 & 0 & 0\\
0 & 0 & 9 & 7 \\
0 & 0 & 2 & 6 \\
\end{array}
\mright]
\end{align*}
\vspace{-.15in}

This is a block diagonal matrix. Why? Because if we break it up this way:

\vspace{-.15in}
\begin{align*}
\mleft[
\begin{array}{cc|cc}
3 & 2 & 0 & 0\\
4 & 4 & 0 & 0\\
\hline
0 & 0 & 9 & 7 \\
0 & 0 & 2 & 6 \\
\end{array}
\mright]
\end{align*}
\vspace{-.15in}

we can see that it satisfies the conditions. The two blocks on the diagonal
(which are the 3--2--4--4 block and the 9--7--2--6 block) are square ($2\times
2$), and all the entries in the off-diagonal blocks are zeroes.

Sometimes it can take a little fiddling around to find the right partition. Try
this one:

\vspace{-.15in}
\begin{align*}
\mleft[
\begin{array}{cccc}
1 & 4 & 3 & 0\\
6 & 8 & 2 & 0\\
5 & 1 & 5 & 0 \\
0 & 0 & 0 & 7 \\
\end{array}
\mright]
\end{align*}
\vspace{-.15in}

Does that one look block diagonal to you? It is:

\vspace{-.15in}
\begin{align*}
\mleft[
\begin{array}{ccc|c}
1 & 4 & 3 & 0\\
6 & 8 & 2 & 0\\
5 & 1 & 5 & 0 \\
\hline
0 & 0 & 0 & 7 \\
\end{array}
\mright]
\end{align*}
\vspace{-.15in}

Notice that the blocks that are \textit{not} on the diagonal don't have to be
square. All that matters are the diagonal blocks, and here we have a $3\times
3$ and a $1\times 1$, so we're good.

Here's one more for the road:

\vspace{-.15in}
\begin{align*}
\mleft[
\begin{array}{ccccccc}
4 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 9 & 0 & 3 & 3 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 2 & 0 & 0 & 0 & 0\\
0 & 7 & 0 & 0 & 5 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 6 & 3\\
0 & 0 & 0 & 0 & 0 & 1 & 0\\
\end{array}
\mright]
\end{align*}
\vspace{-.15in}

Is this one, too, block diagonal? All the extra (and unnecessary) zeroes might
fool you, but if you partition it this way:

\vspace{-.15in}
\begin{align*}
\mleft[
\begin{array}{c|cccc|cc}
4 & 0 & 0 & 0 & 0 & 0 & 0\\
\hline
0 & 9 & 0 & 3 & 3 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 2 & 0 & 0 & 0 & 0\\
0 & 7 & 0 & 0 & 5 & 0 & 0\\
\hline
0 & 0 & 0 & 0 & 0 & 6 & 3\\
0 & 0 & 0 & 0 & 0 & 1 & 0\\
\end{array}
\mright]
\end{align*}
\vspace{-.15in}

you can see that it is.

\vfill
\pagebreak

\subsection*{Answers to ``special'' matrix quiz from
p.~\pageref{specialMatrixQuiz}}

\label{specialMatrixQuizSols}

\begin{multicols}{3}
\begin{compactenum}
\item No.
\item Yes.
\item No.
\item No.
\item Yes.
\item No.
\item Yes.
\item No.
\item Yes.
\item Yes.
\end{compactenum}
\end{multicols}

