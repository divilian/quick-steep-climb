
\chapter{Matrix multiplication}

So far, we've multiplied scalars by vectors
(p.~\pageref{scalarVectorMultiplication}), vectors by other vectors
(p.~\pageref{dotProduct}), scalars by matrices
(p.~\pageref{scalarMatrixMultiplication}), and even matrices by vectors
(p.~\pageref{matrixVectorMultiplication}). The only thing we haven't done yet
is multiply one entire matrix by another. That mysterious operation is the
subject of this chapter.

Luckily, we've already set ourselves up for success. As it will turn out,
matrix-matrix multiplication is really just matrix-\textit{vector}
multiplication ``in a loop''; \textit{i.e.}, repeated several times.

\section{When it's legal and what you get}

But let's not get ahead of ourselves. First, let's outline the very curious
rules for (1) when two matrices \textit{can} be multiplied at all (often they
can't), and (2) if they can, what the dimensions of the result are. These rules
will surprise you at first (they certainly did me).

Let's say we have two matrices called $A$ and $B$. Suppose that $A$ is an
$m\times n$ matrix ($m$ rows and $n$ columns), and that $B$ is a $p\times q$
matrix. Visually, here's what we've got:

\makeatletter
\newcommand\makebig[2]{%
  \@xp\newcommand\@xp*\csname#1\endcsname{\bBigg@{#2}}%
  \@xp\newcommand\@xp*\csname#1l\endcsname{\@xp\mathopen\csname#1\endcsname}%
  \@xp\newcommand\@xp*\csname#1r\endcsname{\@xp\mathclose\csname#1\endcsname}%
}
\makeatother

\makebig{Biggg} {3.5}
\makebig{Bigggg} {4.0}
\makebig{Biggggg} {4.5}

\vspace{-.15in}
\begin{align*}
m \Biggg\{
\overbrace{
\begin{bmatrix}
\smallblacksquare & \smallblacksquare & \cdots & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \cdots & \smallblacksquare \\
\vdots & \vdots & \cdots & \vdots \\
\smallblacksquare & \smallblacksquare & \cdots & \smallblacksquare \\
\end{bmatrix}}^n \ \  \smallblackcircle \ \ 
p \Biggg\{
\overbrace{
\begin{bmatrix}
\smallblacksquare & \smallblacksquare & \cdots & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \cdots & \smallblacksquare \\
\vdots & \vdots & \cdots & \vdots \\
\smallblacksquare & \smallblacksquare & \cdots & \smallblacksquare \\
\end{bmatrix}}^q
\ \ = \ \ \text{\LARGE ?}
\end{align*}
\vspace{-.15in}

\smallskip
Here are the rules:

\begin{center}
\begin{framed}
\begin{compactenum}
\item $n$ must be equal to $p$, or you can't multiply the matrices at all.
\item If $n$ does equal $p$, then you'll get an $m\times q$ matrix when you
multiply them.
\end{compactenum}
\end{framed}
\end{center}

Those rules are so strange and unexpected that it's worth taking a long moment
to stare at both the matrices and the rules and try to digest them.

\smallskip

Some concrete examples:

\begin{enumerate}
\itemsep.5em

\item Can we multiply a $3\times 2$ matrix by a $2\times 4$? Yes, since $n=2$
and $p=2$. And our result will be a $3\times 4$:

\vspace{-.15in}
\begin{align*}
3 \Biggg\{
\overbrace{
\begin{bmatrix}
\smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare \\
\end{bmatrix}}^2 \ \  \cdot \ \ 
2 \Bigg\{
\overbrace{
\begin{bmatrix}
\smallblacksquare & \smallblacksquare & \smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \smallblacksquare & \smallblacksquare \\
\end{bmatrix}}^4
\ \ = \ \ 
3 \Biggg\{
\overbrace{
\begin{bmatrix}
\smallblacksquare & \smallblacksquare & \smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \smallblacksquare & \smallblacksquare \\
\end{bmatrix}}^4.
\end{align*}
\vspace{-.15in}

\item Can we multiply a $2\times 5$ matrix by a $5\times 3$? Yes, since $n=5$
and $p=5$. And we get a $2\times 3$:

\vspace{-.15in}
\begin{align*}
2 \Bigg\{
\overbrace{
\begin{bmatrix}
\smallblacksquare & \smallblacksquare& \smallblacksquare& \smallblacksquare& \smallblacksquare \\
\smallblacksquare & \smallblacksquare& \smallblacksquare& \smallblacksquare& \smallblacksquare \\
\end{bmatrix}}^5 \ \  \cdot \ \ 
5 \Biggggg\{
\overbrace{
\begin{bmatrix}
\smallblacksquare & \smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \smallblacksquare \\
\end{bmatrix}}^3
\ \ = \ \ 
2 \Bigg\{
\overbrace{
\begin{bmatrix}
\smallblacksquare & \smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \smallblacksquare \\
\end{bmatrix}}^3.
\end{align*}
\vspace{-.15in}

\item Can we multiply a $4\times 3$ matrix by another $4\times 3$? No, since
$n=3$ but $p=4$. Sorry.

\vspace{-.15in}
\begin{align*}
4 \Bigggg\{
\overbrace{
\begin{bmatrix}
\smallblacksquare & \smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \smallblacksquare \\
\end{bmatrix}}^3 \ \  \smallblackcircle \ \ 
4 \Bigggg\{
\overbrace{
\begin{bmatrix}
\smallblacksquare & \smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \smallblacksquare \\
\end{bmatrix}}^3
\ \ = \ \ \text{NOPE}.
\end{align*}
\vspace{-.15in}
\end{enumerate}

It's sooo bizarre. Sometimes you multiply two biggish matrices together and get
a small one; sometimes you multiply narrow ones and get a tall one; sometimes
it seems like you'd get a valid answer and yet there is none.

\section{Lather, rinse, repeat}

Now that we have the ground rules for what the resulting matrix will be shaped
like (if there even is one) let's talk about actually calculating the entries.
I'm going to give you \textit{three} different ways to think about this, each
of which sheds a different light on the operation.

\index{matrix-vector multiplication}

The first way is to view the matrix multiplication $A \cdot B$ as
\textbf{repeated matrix-vector multiplication}, where the matrix is $A$ and the
vectors are the \textbf{columns} of $B$. The final answer is formed by
stitching together the results of the individual matrix-vector multiplications.

Let's see it in action. If you remember the procedure on
p.~\pageref{matrixVectorMultiplication}, you can confirm that if we perform
this matrix-vector multiplication:

\vspace{-.15in}
\begin{align*}
\begin{bmatrix}
2 & 1 & 5 \\
0 & 3 & -2 \\
\end{bmatrix} \cdot
\begin{bmatrix}
0 \\ 0 \\ 7 \\
\end{bmatrix},
\end{align*}
\vspace{-.15in}

we'll get the answer

\vspace{-.15in}
\begin{align*}
\begin{bmatrix}
35 \\ -14 \\
\end{bmatrix}.
\end{align*}
\vspace{-.15in}

\smallskip
And if we do this:

\vspace{-.15in}
\begin{align*}
\begin{bmatrix}
2 & 1 & 5 \\
0 & 3 & -2 \\
\end{bmatrix} \cdot
\begin{bmatrix}
9 \\ 99 \\ 999 \\
\end{bmatrix},
\end{align*}
\vspace{-.15in}

we'll get this:

\vspace{-.15in}
\begin{align*}
\begin{bmatrix}
5112 \\ -1701 \\
\end{bmatrix}.
\end{align*}
\vspace{-.15in}

\smallskip
Finally, if we do this:

\vspace{-.15in}
\begin{align*}
\begin{bmatrix}
2 & 1 & 5 \\
0 & 3 & -2 \\
\end{bmatrix} \cdot
\begin{bmatrix}
-13 \\ -13 \\ -13 \\
\end{bmatrix},
\end{align*}
\vspace{-.15in}

we'll get this:

\vspace{-.15in}
\begin{align*}
\begin{bmatrix}
-104 \\ -13 \\
\end{bmatrix}.
\end{align*}
\vspace{-.15in}

\smallskip

Notice what I did there. I took the \textit{same} $2\times 3$ matrix each time,
and multiplied it by some vector -- a weird one, to help jog your memory in a
moment -- to get an answer.

All right. Now let's see what happens if I perform the following
matrix-\textit{matrix} multiplication:

\vspace{-.15in}
\begin{align*}
\begin{bmatrix}
2 & 1 & 5 \\
0 & 3 & -2 \\
\end{bmatrix} \cdot
\begin{bmatrix}
0 & 9 & -13 \\
0 & 99 & -13 \\
7 & 999 & -13 \\
\end{bmatrix} \ = \ {\LARGE ?}
\end{align*}
\vspace{-.15in}

Examine the columns of the right-hand matrix: they should ring a bell. Each
\textit{column} is one of the \textit{vectors} that we just multiplied our
matrix by to get a columnar answer. The result of this operation is achieved by
simply putting all those columnar answers together:

\vspace{-.15in}
\begin{align*}
\begin{bmatrix}
2 & 1 & 5 \\
0 & 3 & -2 \\
\end{bmatrix} \cdot
\begin{bmatrix}
0 & 9 & -13 \\
0 & 99 & -13 \\
7 & 999 & -13 \\
\end{bmatrix} =
\begin{bmatrix}
35 & 5112 & -104 \\
-14 & -1701 & -13 \\
\end{bmatrix}.
\end{align*}
\vspace{-.15in}

\index{Bond, James}

See how that works? The result of the multiplication is just the three
individual matrix-vector products, all concatenated together in an ``answer
matrix.'' The left column of our answer is ${\scriptsize \begin{bmatrix} 35 \\
-14 \\ \end{bmatrix}}$, which is exactly what we got when we multiplied that
left-hand matrix by James Bond. The right column of our answer is ${\scriptsize
\begin{bmatrix} -104 \\ -13 \\ \end{bmatrix}}$, which is what we got when we
multiplied the matrix by triple $-13$'s. And the middle column of the answer is
the matrix times the stack of nines. So you can see that matrix-\textit{matrix}
multiplication is really just repeated matrix-\textit{vector} multiplication.

This way of thinking about matrix multiplication might be the one that
resonates most strongly with you. (It did for me.)

\section{All possible dot products}

On the other hand, maybe you'll like this one better. Matrix-matrix
multiplication can also be viewed as \textbf{all possible dot products} between
the \textbf{rows} of $A$ and the \textbf{columns} of $B$.

Flash back for a moment to \textit{A Cool, Brisk Walk} chapter 6, and the
Fundamental Theorem of Counting. Answer this question: ``You have two choices
of appetizer, and three choices of entr\'{e}e. How many different dinner
combinations are possible?''

The answer is six, since each of the two appetizers can go with any of the
three entr\'{e}es. So you could choose:

\begin{compactenum}
\item shrimp cocktail, filet mignon
\item shrimp cocktail, chicken pesto
\item shrimp cocktail, eggplant parmigiana
\item artichoke dip, filet mignon
\item artichoke dip, chicken pesto
\item artichoke dip, eggplant parmigiana
\end{compactenum}

Now back to matrices. If I multiply these two matrices together:

\vspace{-.15in}
\begin{align*}
\begin{bmatrix}
2 & 1 & 5 \\
0 & 3 & -2 \\
\end{bmatrix} \ \text{and} \ 
\begin{bmatrix}
0 & 9 & -13 \\
0 & 99 & -13 \\
7 & 999 & -13 \\
\end{bmatrix},
\end{align*}
\vspace{-.15in}

how many possible dot products are there between \textit{rows} of $A$ and
\textit{columns} of $B$?

The answer is six, since each of the two $A$ rows can go with any of the three
$B$ rows. The possibilities are:

\begin{compactenum}
\item $[\ 2\ \ 1\ \ 5\ ]$ and $[\ 0\ \ 0\ \ 7\ ]$
\item $[\ 2\ \ 1\ \ 5\ ]$ and $[\ 9\ \ 99\ \ 999\ ]$
\item $[\ 2\ \ 1\ \ 5\ ]$ and $[\ -13\ \ -13\ \ -13\ ]$
\item $[\ 0\ \ 3\ \ -2\ ]$ and $[\ 0\ \ 0\ \ 7\ ]$
\item $[\ 0\ \ 3\ \ -2\ ]$ and $[\ 9\ \ 99\ \ 999\ ]$
\item $[\ 0\ \ 3\ \ -2\ ]$ and $[\ -13\ \ -13\ \ -13\ ]$
\end{compactenum}

(Note \textit{very} carefully that we use the \textit{columns} of $B$, not the
rows!)

\smallskip
Very well. Let's compute all those dot products then:

\begin{compactitem}
\item $[\ 2\ \ 1\ \ 5\ ] \cdot [\ 0\ \ 0\ \ 7\ ] = 35$
\item $[\ 2\ \ 1\ \ 5\ ] \cdot [\ 9\ \ 99\ \ 999\ ] = 5112$
\item $[\ 2\ \ 1\ \ 5\ ] \cdot [\ -13\ \ -13\ \ -13\ ] = -104$
\item $[\ 0\ \ 3\ \ -2\ ] \cdot [\ 0\ \ 0\ \ 7\ ] = -14$
\item $[\ 0\ \ 3\ \ -2\ ] \cdot [\ 9\ \ 99\ \ 999\ ] = -1701$
\item $[\ 0\ \ 3\ \ -2\ ] \cdot [\ -13\ \ -13\ \ -13\ ] = -13$
\end{compactitem}

\smallskip
Those six dot products are precisely the entries in our answer matrix:

\vspace{-.15in}
\begin{align*}
\begin{bmatrix}
35 & 5112 & -104 \\
-14 & -1701 & -13 \\
\end{bmatrix}.
\end{align*}
\vspace{-.15in}

The only thing you have to be careful of is which answer goes in which place.
The rule is:

\begin{framed}
The dot product of row $i$ of $A$ and column $j$ of $B$ goes in
row $i$, column $j$ of the answer.
\end{framed}

A sensible arrangement, I think you'll agree. Multiplying row 0 with column 0
will give us the entry in row 0, column 0 of our answer. Multiplying row 14
with column 9 will give us the entry in row 14, column 9 of our answer. And so
forth.

In terms of our current example, the reason that the number 5112 goes in the
\textit{top middle} of our answer (as opposed to the bottom left, or anywhere
else) is that 5112 is the dot product of the \textit{top} row of $A$ ($[\ 2\ \
1\ \ 5\ ]$) with the \textit{middle} column of $B$ ($[\ 9\ \ 99\ \ 999\ ]$).
Be sure to practice with this so you don't get numbers out of place.

\smallskip

\index{matchmaker@\texttt{matchmaker.com}}

It might help to keep in mind possible applications here. Why would we ever
want to compute ``all possible dot products?'' Well, think back to our
matchmaker example. Let's say we have 4 women and 5 men, each of whom has
completed a survey. Finding all the compatibilities -- \textit{i.e.},
predicting the dating success of all possible pairings -- is precisely
computing the dot product of every gal with every guy (assuming
heterosexuality). That's 20 possible dot products, which we can calculate with
a single matrix multiplication.

% TODO: explain that we need "Women times TRANSPOSE of men" not "women times
% men."

\section{Several different linear combinations}

Our third and final way to think about matrix multiplication is in terms of
linear combinations. Remember (from p.~\pageref{linearComboOfColumns}) that
every matrix-\textit{vector} multiplication $A \cdot
\overrightarrow{\textbf{x}}$ is essentially specifying some linear combination
of $A$'s \textit{columns}. If we multiply $A$ by the vector ${\scriptsize
\begin{bmatrix} 3 \\ 5 \\ \end{bmatrix}}$, we're saying ``I'd like 3 copies of
$A$'s first column, plus 5 copies of its second column, please.''

Matrix multiplication is simply asking for several \textit{different} linear
combinations. If we multiply a matrix $A$ by this one:

\vspace{-.15in}
\begin{align*}
\begin{bmatrix}
0 & 9 & -13 \\
0 & 99 & -13 \\
7 & 999 & -13 \\
\end{bmatrix},
\end{align*}
\vspace{-.15in}

we're requesting the following:

\begin{quote}
\index{credit card}
\textit{
``Hello, I'd like to put in an order for three things. First, I'd like 7 copies
of $A$'s third column (ignore the first two). Additionally, I'd like 9 copies
of its first column, 99 copies of its second column, and 999 copies of its
third column, all added together. Finally, please give me $-13$ copies of each
of its columns, again added together. Thanks! You should have my credit card
number on file.''
}
\end{quote}

To fulfill this order, we compute each of the three linear combinations
requested. Using the same $A$ matrix we've been using (${\scriptsize
\begin{bmatrix} 2 & 1 & 5 \\ 0 & 3 & -2 \end{bmatrix}}$) this amounts to:

\vspace{-.15in}
\begin{align*}
\text{First combination: } &7
\begin{bmatrix}
5 \\
-2 \\
\end{bmatrix} =
\begin{bmatrix}
35 \\
-14 \\
\end{bmatrix} \\
\text{Second combination: } &9
\begin{bmatrix}
2 \\
0 \\
\end{bmatrix} + 99
\begin{bmatrix}
1 \\
3 \\
\end{bmatrix} + 999
\begin{bmatrix}
5 \\
-2 \\
\end{bmatrix} =
\begin{bmatrix}
5112 \\
-1701 \\
\end{bmatrix} \\
\text{Third combination: } &-13
\begin{bmatrix}
2 \\
0 \\
\end{bmatrix} -13
\begin{bmatrix}
1 \\
3 \\
\end{bmatrix} -13
\begin{bmatrix}
5 \\
-2 \\
\end{bmatrix} =
\begin{bmatrix}
-104 \\
-13 \\
\end{bmatrix}
\end{align*}
\vspace{-.15in}

Packaging up all those results again gives us:

\vspace{-.15in}
\begin{align*}
\begin{bmatrix}
35 & 5112 & -104 \\
-14 & -1701 & -13 \\
\end{bmatrix}.
\end{align*}
\vspace{-.15in}


%\subsection{Outer and inner products}
%
%Because we can treat a vector as a sort of degenerate matrix (with only one
%row, or only one column), it sometimes makes sense to do this matrix-vector
%multiplication with two vectors. Which one is treated as a row vector and which
%one is treated as a column vector makes all the difference.
%
%As an illustration, I'm going to define vectors $\overrightarrow{\textbf{p}}$
%and $\overrightarrow{\textbf{q}}$ this way:
%
%\vspace{-.15in}
%\begin{align*}
%\overrightarrow{\textbf{p}} =
%\begin{bmatrix}
%3 & 1 & 2 \\
%\end{bmatrix}, \quad 
%\overrightarrow{\textbf{q}} =
%\begin{bmatrix}
%5 \\  4 \\ -3 \\
%\end{bmatrix}.
%\end{align*}
%\vspace{-.15in}
%
%So $\overrightarrow{\textbf{p}}$ is a row vector, and
%$\overrightarrow{\textbf{q}}$ is a column vector.
%
%Now, treating $\overrightarrow{\textbf{p}}$ as a $1\times 3$ matrix, we perform
%multiplication and get:
%
%\vspace{-.15in}
%\begin{align*}
%\overrightarrow{\textbf{p}} \cdot \overrightarrow{\textbf{q}} =
%\begin{bmatrix}
%3 & 1 & 2 \\
%\end{bmatrix} \cdot
%\begin{bmatrix}
%5 \\ 4 \\ -3 \\
%\end{bmatrix} = 13.
%\end{align*}
%\vspace{-.15in}
%
%\index{inner product}
%It's just the dot product, of course, calculated in the usual way. Here, for
%reasons that will shortly become clear, we also call it the \textbf{inner
%product} of the two vectors.
%
%Now suppose I swap the order, and compute $\overrightarrow{\textbf{q}}$ times
%$\overrightarrow{\textbf{p}}$ instead. What would I get? The answer will surely
%surprise you:
%
%\vspace{-.15in}
%\begin{align*}
%\overrightarrow{\textbf{q}} \cdot \overrightarrow{\textbf{p}} =
%\begin{bmatrix}
%5 \\ 4 \\ -3 \\
%\end{bmatrix} \cdot
%\begin{bmatrix}
%3 & 1 & 2 \\
%\end{bmatrix} =
%\begin{bmatrix}
%15 & 5 & 10 \\
%12 & 4 & 8 \\
%-9 & -3 & -6 \\
%\end{bmatrix}.
%\end{align*}
%\vspace{-.15in}
%
%Hooooooo...\textit{what?!} $\overrightarrow{\textbf{p}}$ times
%$\overrightarrow{\textbf{q}}$ is the number 13, but
%$\overrightarrow{\textbf{q}}$ times $\overrightarrow{\textbf{p}}$ is an entire
%grid full of numbers?
%
%Yes it is. Here's why. 




% Recall from linear ops chapter. We can have multiple operators all strung together!
% Scaling [ 4 0 ; 0 4 ]  x  rotation[ sqrt(2) ]  x  flip[ -1 0 ; 0 1 ]
% associativity means we can compute a combined operator all in advance.
