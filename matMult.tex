
\chapter{Matrix multiplication}

So far, we've multiplied scalars by vectors
(p.~\pageref{scalarVectorMultiplication}), vectors by other vectors
(p.~\pageref{dotProduct}), scalars by matrices
(p.~\pageref{scalarMatrixMultiplication}), and even matrices by vectors
(p.~\pageref{matrixVectorMultiplication}). The only thing we haven't done yet
is multiply one entire matrix by another. That mysterious operation is the
subject of this chapter.

Luckily, we've already set ourselves up for success. As it will turn out,
matrix-matrix multiplication is really just matrix-\textit{vector}
multiplication ``in a loop''; \textit{i.e.}, repeated several times.

\section{When it's legal and what you get}

But let's not get ahead of ourselves. First, let's outline the very curious
rules for (1) when two matrices \textit{can} be multiplied at all (often they
can't), and (2) if they can, what the dimensions of the result are. These rules
will surprise you at first (they certainly did me).

Let's say we have two matrices called $A$ and $B$. Suppose that $A$ is an
$m\times n$ matrix ($m$ rows and $n$ columns), and that $B$ is a $p\times q$
matrix. Visually, here's what we've got:

\makeatletter
\newcommand\makebig[2]{%
  \@xp\newcommand\@xp*\csname#1\endcsname{\bBigg@{#2}}%
  \@xp\newcommand\@xp*\csname#1l\endcsname{\@xp\mathopen\csname#1\endcsname}%
  \@xp\newcommand\@xp*\csname#1r\endcsname{\@xp\mathclose\csname#1\endcsname}%
}
\makeatother

\makebig{Biggg} {3.5}
\makebig{Bigggg} {4.0}
\makebig{Biggggg} {4.5}

\vspace{-.15in}
\begin{align*}
m \Biggg\{
\overbrace{
\begin{bmatrix}
\smallblacksquare & \smallblacksquare & \cdots & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \cdots & \smallblacksquare \\
\vdots & \vdots & \cdots & \vdots \\
\smallblacksquare & \smallblacksquare & \cdots & \smallblacksquare \\
\end{bmatrix}}^n \ \  \smallblackcircle \ \ 
p \Biggg\{
\overbrace{
\begin{bmatrix}
\smallblacksquare & \smallblacksquare & \cdots & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \cdots & \smallblacksquare \\
\vdots & \vdots & \cdots & \vdots \\
\smallblacksquare & \smallblacksquare & \cdots & \smallblacksquare \\
\end{bmatrix}}^q
\ \ = \ \ \text{\LARGE ?}
\end{align*}
\vspace{-.15in}

\smallskip
Here are the rules:

\begin{center}
\begin{framed}
\begin{compactenum}
\label{matMultRules}
\item $n$ must be equal to $p$, or you can't multiply the matrices at all.
\item If $n$ does equal $p$, then you'll get an $m\times q$ matrix when you
multiply them.
\end{compactenum}
\end{framed}
\end{center}

Those rules are so strange and unexpected that it's worth taking a long moment
to stare at both the matrices and the rules and try to digest them.

\smallskip

Some concrete examples:

\begin{enumerate}
\itemsep.5em

\item Can we multiply a $3\times 2$ matrix by a $2\times 4$? Yes, since $n=2$
and $p=2$. And our result will be a $3\times 4$:

\vspace{-.15in}
\begin{align*}
3 \Biggg\{
\overbrace{
\begin{bmatrix}
\smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare \\
\end{bmatrix}}^2 \ \  \cdot \ \ 
2 \Bigg\{
\overbrace{
\begin{bmatrix}
\smallblacksquare & \smallblacksquare & \smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \smallblacksquare & \smallblacksquare \\
\end{bmatrix}}^4
\ \ = \ \ 
3 \Biggg\{
\overbrace{
\begin{bmatrix}
\smallblacksquare & \smallblacksquare & \smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \smallblacksquare & \smallblacksquare \\
\end{bmatrix}}^4.
\end{align*}
\vspace{-.15in}

\item Can we multiply a $2\times 5$ matrix by a $5\times 3$? Yes, since $n=5$
and $p=5$. And we get a $2\times 3$:

\vspace{-.15in}
\begin{align*}
2 \Bigg\{
\overbrace{
\begin{bmatrix}
\smallblacksquare & \smallblacksquare& \smallblacksquare& \smallblacksquare& \smallblacksquare \\
\smallblacksquare & \smallblacksquare& \smallblacksquare& \smallblacksquare& \smallblacksquare \\
\end{bmatrix}}^5 \ \  \cdot \ \ 
5 \Biggggg\{
\overbrace{
\begin{bmatrix}
\smallblacksquare & \smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \smallblacksquare \\
\end{bmatrix}}^3
\ \ = \ \ 
2 \Bigg\{
\overbrace{
\begin{bmatrix}
\smallblacksquare & \smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \smallblacksquare \\
\end{bmatrix}}^3.
\end{align*}
\vspace{-.15in}

\item Can we multiply a $4\times 3$ matrix by another $4\times 3$? No, since
$n=3$ but $p=4$. Sorry.

\vspace{-.15in}
\begin{align*}
4 \Bigggg\{
\overbrace{
\begin{bmatrix}
\smallblacksquare & \smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \smallblacksquare \\
\end{bmatrix}}^3 \ \  \smallblackcircle \ \ 
4 \Bigggg\{
\overbrace{
\begin{bmatrix}
\smallblacksquare & \smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \smallblacksquare \\
\end{bmatrix}}^3
\ \ = \ \ \text{NOPE}.
\end{align*}
\vspace{-.15in}
\end{enumerate}

It's sooo bizarre. Sometimes you multiply two biggish matrices together and get
a small one; sometimes you multiply narrow ones and get a tall one; sometimes
it seems like you'd get a valid answer and yet there is none.

Anyway, now that we have the ground rules for what the resulting matrix will be
shaped like (if there even is one) let's talk about actually calculating the
entries. I'm going to give you \textit{three} different ways to think about
this, each of which sheds a different light on the operation.

\section{Way \#1: Lather, rinse, repeat}

\index{matrix-vector multiplication}

The first way is to view the matrix multiplication $A \cdot B$ as
\textbf{repeated matrix-vector multiplication}, where the matrix is $A$ and the
vectors are the \textbf{columns} of $B$. The final answer is formed by
stitching together the results of the individual matrix-vector multiplications.

Let's see it in action. If you remember the procedure on
p.~\pageref{matrixVectorMultiplication}, you can confirm that if we perform
this matrix-vector multiplication:

\vspace{-.15in}
\begin{align*}
\begin{bmatrix}
2 & 1 & 5 \\
0 & 3 & -2 \\
\end{bmatrix} \cdot
\begin{bmatrix}
0 \\ 0 \\ 7 \\
\end{bmatrix},
\end{align*}
\vspace{-.15in}

we'll get the answer

\vspace{-.15in}
\begin{align*}
\begin{bmatrix}
35 \\ -14 \\
\end{bmatrix}.
\end{align*}
\vspace{-.15in}

\smallskip
And if we do this:

\vspace{-.15in}
\begin{align*}
\begin{bmatrix}
2 & 1 & 5 \\
0 & 3 & -2 \\
\end{bmatrix} \cdot
\begin{bmatrix}
9 \\ 99 \\ 999 \\
\end{bmatrix},
\end{align*}
\vspace{-.15in}

we'll get this:

\vspace{-.15in}
\begin{align*}
\begin{bmatrix}
5112 \\ -1701 \\
\end{bmatrix}.
\end{align*}
\vspace{-.15in}

\smallskip
Finally, if we do this:

\vspace{-.15in}
\begin{align*}
\begin{bmatrix}
2 & 1 & 5 \\
0 & 3 & -2 \\
\end{bmatrix} \cdot
\begin{bmatrix}
-13 \\ -13 \\ -13 \\
\end{bmatrix},
\end{align*}
\vspace{-.15in}

we'll get this:

\vspace{-.15in}
\begin{align*}
\begin{bmatrix}
-104 \\ -13 \\
\end{bmatrix}.
\end{align*}
\vspace{-.15in}

\smallskip

Notice what I did there. I took the \textit{same} $2\times 3$ matrix each time,
and multiplied it by some vector -- a weird one, to help jog your memory in a
moment -- to get an answer.

All right. Now let's see what happens if I perform the following
matrix-\textit{matrix} multiplication:

\vspace{-.15in}
\begin{align*}
\begin{bmatrix}
2 & 1 & 5 \\
0 & 3 & -2 \\
\end{bmatrix} \cdot
\begin{bmatrix}
0 & 9 & -13 \\
0 & 99 & -13 \\
7 & 999 & -13 \\
\end{bmatrix} \ = \ {\LARGE ?}
\end{align*}
\vspace{-.15in}

Examine the columns of the right-hand matrix: they should ring a bell. Each
\textit{column} is one of the \textit{vectors} that we just multiplied our
matrix by to get a columnar answer. The result of this operation is achieved by
simply putting all those columnar answers together:

\vspace{-.15in}
\begin{align*}
\begin{bmatrix}
2 & 1 & 5 \\
0 & 3 & -2 \\
\end{bmatrix} \cdot
\begin{bmatrix}
0 & 9 & -13 \\
0 & 99 & -13 \\
7 & 999 & -13 \\
\end{bmatrix} =
\begin{bmatrix}
35 & 5112 & -104 \\
-14 & -1701 & -13 \\
\end{bmatrix}.
\end{align*}
\vspace{-.15in}

\index{Bond, James}

See how that works? The result of the multiplication is just the three
individual matrix-vector products, all concatenated together in an ``answer
matrix.'' The left column of our answer is ${\scriptsize \begin{bmatrix} 35 \\
-14 \\ \end{bmatrix}}$, which is exactly what we got when we multiplied that
left-hand matrix by James Bond. The right column of our answer is ${\scriptsize
\begin{bmatrix} -104 \\ -13 \\ \end{bmatrix}}$, which is what we got when we
multiplied the matrix by triple $-13$'s. And the middle column of the answer is
the matrix times the stack of nines. So you can see that matrix-\textit{matrix}
multiplication is really just repeated matrix-\textit{vector} multiplication.

This way of thinking about matrix multiplication might be the one that
resonates most strongly with you. (It did for me.)

\section{Way \#2: All possible dot products}

On the other hand, maybe you'll like this one better. Matrix-matrix
multiplication can also be viewed as \textbf{all possible dot products} between
the \textbf{rows} of $A$ and the \textbf{columns} of $B$.

Flash back for a moment to \textit{A Cool, Brisk Walk} chapter 6, and the
Fundamental Theorem of Counting. Answer this question: ``You have two choices
of appetizer, and three choices of entr\'{e}e. How many different dinner
combinations are possible?''

The answer is six, since each of the two appetizers can go with any of the
three entr\'{e}es. So you could choose:

\begin{compactenum}
\item shrimp cocktail, filet mignon
\item shrimp cocktail, chicken pesto
\item shrimp cocktail, eggplant parmigiana
\item artichoke dip, filet mignon
\item artichoke dip, chicken pesto
\item artichoke dip, eggplant parmigiana
\end{compactenum}

Now back to matrices. If I multiply these two matrices together:

\vspace{-.15in}
\begin{align*}
\begin{bmatrix}
2 & 1 & 5 \\
0 & 3 & -2 \\
\end{bmatrix} \ \text{and} \ 
\begin{bmatrix}
0 & 9 & -13 \\
0 & 99 & -13 \\
7 & 999 & -13 \\
\end{bmatrix},
\end{align*}
\vspace{-.15in}

how many possible dot products are there between \textit{rows} of $A$ and
\textit{columns} of $B$?

The answer is six, since each of the two $A$ rows can go with any of the three
$B$ rows. The possibilities are:

\begin{compactenum}
\item $[\ 2\ \ 1\ \ 5\ ]$ and $[\ 0\ \ 0\ \ 7\ ]$
\item $[\ 2\ \ 1\ \ 5\ ]$ and $[\ 9\ \ 99\ \ 999\ ]$
\item $[\ 2\ \ 1\ \ 5\ ]$ and $[\ -13\ \ -13\ \ -13\ ]$
\item $[\ 0\ \ 3\ \ -2\ ]$ and $[\ 0\ \ 0\ \ 7\ ]$
\item $[\ 0\ \ 3\ \ -2\ ]$ and $[\ 9\ \ 99\ \ 999\ ]$
\item $[\ 0\ \ 3\ \ -2\ ]$ and $[\ -13\ \ -13\ \ -13\ ]$
\end{compactenum}

(Note \textit{very} carefully that we use the \textit{columns} of $B$, not the
rows!)

\smallskip
Very well. Let's compute all those dot products then:

\begin{compactitem}
\item $[\ 2\ \ 1\ \ 5\ ] \cdot [\ 0\ \ 0\ \ 7\ ] = 35$
\item $[\ 2\ \ 1\ \ 5\ ] \cdot [\ 9\ \ 99\ \ 999\ ] = 5112$
\item $[\ 2\ \ 1\ \ 5\ ] \cdot [\ -13\ \ -13\ \ -13\ ] = -104$
\item $[\ 0\ \ 3\ \ -2\ ] \cdot [\ 0\ \ 0\ \ 7\ ] = -14$
\item $[\ 0\ \ 3\ \ -2\ ] \cdot [\ 9\ \ 99\ \ 999\ ] = -1701$
\item $[\ 0\ \ 3\ \ -2\ ] \cdot [\ -13\ \ -13\ \ -13\ ] = -13$
\end{compactitem}

\smallskip
Those six dot products are precisely the entries in our answer matrix:

\vspace{-.15in}
\begin{align*}
\begin{bmatrix}
35 & 5112 & -104 \\
-14 & -1701 & -13 \\
\end{bmatrix}.
\end{align*}
\vspace{-.15in}

The only thing you have to be careful of is which answer goes in which place.
The rule is:

\begin{framed}
The dot product of row $i$ of $A$ and column $j$ of $B$ goes in
row $i$, column $j$ of the answer.
\end{framed}

A sensible arrangement, I think you'll agree. Multiplying row 0 with column 0
will give us the entry in row 0, column 0 of our answer. Multiplying row 14
with column 9 will give us the entry in row 14, column 9 of our answer. And so
forth.

In terms of our current example, the reason that the number 5112 goes in the
\textit{top middle} of our answer (as opposed to the bottom left, or anywhere
else) is that 5112 is the dot product of the \textit{top} row of $A$ ($[\ 2\ \
1\ \ 5\ ]$) with the \textit{middle} column of $B$ ($[\ 9\ \ 99\ \ 999\ ]$).
Be sure to practice with this so you don't get numbers out of place.

\smallskip

\index{matchmaker@\texttt{matchmaker.com}}

It might help to keep in mind possible applications here. Why would we ever
want to compute ``all possible dot products?'' Well, think back to our
matchmaker example. Let's say we have 4 women and 5 men, each of whom has
completed a survey. Finding all the compatibilities -- \textit{i.e.},
predicting the dating success of all possible pairings -- is precisely
computing the dot product of every gal with every guy (assuming
heterosexuality). That's 20 possible dot products, which we can calculate with
a single matrix multiplication.

% TODO: explain that we need "Women times TRANSPOSE of men" not "women times
% men."

\section{Way \#3: Several linear combinations}

Our third and final way to think about matrix multiplication is in terms of
linear combinations. Remember (from p.~\pageref{linearComboOfColumns}) that
every matrix-\textit{vector} multiplication $A \cdot
\overrightarrow{\textbf{x}}$ is essentially specifying some linear combination
of $A$'s \textit{columns}. If we multiply $A$ by the vector ${\scriptsize
\begin{bmatrix} 3 \\ 5 \\ \end{bmatrix}}$, we're saying ``I'd like 3 copies of
$A$'s first column, plus 5 copies of its second column, please.''

Matrix multiplication is simply asking for several \textit{different} linear
combinations. If we multiply a matrix $A$ by this one:

\vspace{-.15in}
\begin{align*}
\begin{bmatrix}
0 & 9 & -13 \\
0 & 99 & -13 \\
7 & 999 & -13 \\
\end{bmatrix},
\end{align*}
\vspace{-.15in}

we're requesting the following:

\begin{quote}
\index{credit card}
\textit{
``Hello, I'd like to put in an order for three things. First, I'd like 7 copies
of $A$'s third column (ignore the first two). Additionally, I'd like 9 copies
of its first column, 99 copies of its second column, and 999 copies of its
third column, all added together. Finally, please give me $-13$ copies of each
of its columns, again added together. Thanks! You should have my credit card
number on file.''
}
\end{quote}

To fulfill this order, we compute each of the three linear combinations
requested. Using the same $A$ matrix we've been using (${\scriptsize
\begin{bmatrix} 2 & 1 & 5 \\ 0 & 3 & -2 \end{bmatrix}}$) this amounts to:

\vspace{-.15in}
\begin{align*}
\text{First combination: } &7
\begin{bmatrix}
5 \\
-2 \\
\end{bmatrix} =
\begin{bmatrix}
35 \\
-14 \\
\end{bmatrix} \\
\text{Second combination: } &9
\begin{bmatrix}
2 \\
0 \\
\end{bmatrix} + 99
\begin{bmatrix}
1 \\
3 \\
\end{bmatrix} + 999
\begin{bmatrix}
5 \\
-2 \\
\end{bmatrix} =
\begin{bmatrix}
5112 \\
-1701 \\
\end{bmatrix} \\
\text{Third combination: } &-13
\begin{bmatrix}
2 \\
0 \\
\end{bmatrix} -13
\begin{bmatrix}
1 \\
3 \\
\end{bmatrix} -13
\begin{bmatrix}
5 \\
-2 \\
\end{bmatrix} =
\begin{bmatrix}
-104 \\
-13 \\
\end{bmatrix}
\end{align*}
\vspace{-.15in}

Packaging up all those results again gives us:

\vspace{-.15in}
\begin{align*}
\begin{bmatrix}
35 & 5112 & -104 \\
-14 & -1701 & -13 \\
\end{bmatrix}.
\end{align*}
\vspace{-.15in}

Same answer no matter which of the three ways we think about it.

\section{Outer and inner products}

All right. Now for some surprises.

\index{row vector}
\index{column vector}

Remember (p.~\pageref{rowAndColVectors}) that we will sometimes want to treat a
vector as a sort of degenerate matrix: a matrix with only one row, or only one
column. And we will sometimes want to do this matrix multiplication thing with
two vectors, treating one of them as a row vector and the other as a column
vector. Which one is which makes a tremendous difference.

As an illustration, I'm going to define vectors $\overrightarrow{\textbf{x}}$
and $\overrightarrow{\textbf{y}}$ this way:

\vspace{-.15in}
\begin{align*}
\overrightarrow{\textbf{x}} =
\begin{bmatrix}
3 & 1 & 2 \\
\end{bmatrix}, \quad 
\overrightarrow{\textbf{y}} =
\begin{bmatrix}
5 \\  4 \\ -3 \\
\end{bmatrix}.
\end{align*}
\vspace{-.15in}

So $\overrightarrow{\textbf{x}}$ is a row vector, and
$\overrightarrow{\textbf{y}}$ is a column vector. Put another way,
$\overrightarrow{\textbf{x}}$ can be thought of as a $1\times 3$ matrix, and
$\overrightarrow{\textbf{y}}$ as a $3\times 1$ matrix.

Now if we do treat these as matrices, then performing the operation
$\overrightarrow{\textbf{x}} \cdot \overrightarrow{\textbf{y}}$ gives us:

\vspace{-.15in}
\begin{align*}
\overrightarrow{\textbf{x}} \cdot \overrightarrow{\textbf{y}} =
\begin{bmatrix}
3 & 1 & 2 \\
\end{bmatrix} \cdot
\begin{bmatrix}
5 \\ 4 \\ -3 \\
\end{bmatrix} = 13.
\end{align*}
\vspace{-.15in}

It's just the dot product, of course, calculated in the usual way.

Now suppose I swap the order, and compute $\overrightarrow{\textbf{y}}$ times
$\overrightarrow{\textbf{x}}$ instead. What would I get? The answer will surely
surprise you:

\vspace{-.15in}
\begin{align*}
\overrightarrow{\textbf{y}} \cdot \overrightarrow{\textbf{x}} =
\begin{bmatrix}
5 \\ 4 \\ -3 \\
\end{bmatrix} \cdot
\begin{bmatrix}
3 & 1 & 2 \\
\end{bmatrix} =
\begin{bmatrix}
15 & 5 & 10 \\
12 & 4 & 8 \\
-9 & -3 & -6 \\
\end{bmatrix}.
\end{align*}
\vspace{-.15in}

Hooooooo...wut?! $\overrightarrow{\textbf{x}}$ times
$\overrightarrow{\textbf{y}}$ is the \textit{number} 13, but
$\overrightarrow{\textbf{y}}$ times $\overrightarrow{\textbf{x}}$ is an entire
\textit{grid} full of numbers?

Yes it is. Here's why. 

Remember our rules from p.~\pageref{matMultRules}. First, we can only multiply
two matrices if $n=p$. And that's true here whether we do
$\overrightarrow{\textbf{x}} \cdot \overrightarrow{\textbf{y}}$ or
$\overrightarrow{\textbf{y}} \cdot \overrightarrow{\textbf{x}}$. But the second
rule tells us that the answer will be a $m\times q$ matrix. If we put
$\overrightarrow{\textbf{x}}$ on the left, then $\overrightarrow{\textbf{x}}
\cdot \overrightarrow{\textbf{y}}$ will give us a $1\times 1$ matrix. But if we
put $\overrightarrow{\textbf{y}}$ on the left, then
$\overrightarrow{\textbf{y}} \cdot \overrightarrow{\textbf{x}}$ must give us a
$3\times 3$ matrix. Strange but true.

\index{inner product}
\index{outer product}

Btw, when we do the first thing -- treat the vector on the left-hand side of
the multiplication as a row vector, and the other as a column vector -- it's
called the \textbf{inner product} of the two vectors. The other way is called
the \textbf{outer product}.

\section{$A\cdot A^\intercal$ vs.~$A^\intercal \cdot A$} 

\index{transpose}

Here's another interesting consequence of our operation. As we've seen, you
certainly can't multiply a matrix $A$ by just ``any old thing,'' since rule 1
on p.~\pageref{matMultRules} says that $n$ must equal $p$.

You can, however, \textit{always} perform the operation $A\cdot A^\intercal$,
no matter what dimensions $A$ is. That's because if $A$ is, say, $17\times 28$,
then $A^\intercal$ will be $28\times 17$, and $n=p$ (both are 28) as required.
You'll get a $17\times 17$ matrix if you do that.

You also can \textit{always} perform the operation $A^\intercal \cdot A$.
Again, if $A$ is $17\times 28$, then $A^\intercal$ will be $28\times 17$, and
so again $n=p$ (both are 17). You'll get a $28\times 28$ matrix if you do that.

Here's an example, smaller so it fits on the page. Let's say $A$ is

\vspace{-.15in}
\begin{align*}
\begin{bmatrix}
2 & 20 & 3 & -2 & -4 \\
-5 & 1 & 4 & 1 & 9 \\
\end{bmatrix}.
\end{align*}
\vspace{-.15in}

The two operations give us:

\vspace{-.15in}
\begin{align*}
A\cdot A^\intercal = 
\begin{bmatrix}
2 & 20 & 3 & -2 & -4 \\
-5 & 1 & 4 & 1 & 9 \\
\end{bmatrix} \cdot
\begin{bmatrix}
2 & -5 \\
20 & 1 \\
3 & 4 \\
-2 & 1 \\
-4 & 9 \\
\end{bmatrix}  =
\begin{bmatrix}
433 & -16 \\
-16 & 124 \\
\end{bmatrix},
\end{align*}
\vspace{-.15in}

and 

\vspace{-.15in}
\begin{align*}
A^\intercal \cdot A = 
\begin{bmatrix}
2 & -5 \\
20 & 1 \\
3 & 4 \\
-2 & 1 \\
-4 & 9 \\
\end{bmatrix} \cdot
\begin{bmatrix}
2 & 20 & 3 & -2 & -4 \\
-5 & 1 & 4 & 1 & 9 \\
\end{bmatrix} = \\
\begin{bmatrix}
29 & 35 & -14 & -9 & -53 \\
35 & 401 & 64 & -39 & -71 \\
-14 & 64 & 25 & -2 & 24 \\
-9 & -39 & -2 & 5 & 17 \\
-53 & -71 & 24 & 17 & 97 \\
\end{bmatrix}.
\end{align*}
\vspace{-.15in}

\index{symmetric matrix}

Two other intriguing facts are worth noting here, one of which you may have
noticed. If you run your eyeballs carefully over those two results, you'll see
that both of them are \textbf{symmetric matrices}. This is always true of a
matrix times its transpose, and that turns out to be important for some
applications.

The other fact -- certainly not ascertainable to my eyeballs, at least -- is
that both of these matrices have only \textit{rank 2}. That's not surprising
about $A\cdot A^\intercal$, since it's just a $2\times 2$ anyway. But it is
very surprising about $A^\intercal \cdot A$. That's a $5\times 5$ matrix with
only \textit{two} linearly independent columns!

And this is always true. When you multiply a matrix by its transpose, either
way you do it, the rank will only be the \textit{lower} of the two dimensions.

The way I think of it is this. When you take a tall matrix (like our $2\times
5$, above) and multiply it by a wide one (the $5\times 2$), yes you're going to
get a result with large dimensions. Sure. But in a way, you only put ``two
columns' worth'' of information into the operation. The result is a large
$5\times 5$, but that's misleading, because there just isn't enough information
present in those 25 entries to represent five independent directions. The
$5\times 5$ result is brittle, containing only two columns' worth of
information spread out over a large landscape. It's almost as if I wrote a
fourteen-sentence paragraph with only three sentences repeated again and again,
with the words rescrambled a bit each time. Sure, it looks like a long
paragraph at first glance, but try reading it and you'll recognize how little
information it really contains.



% Recall from linear ops chapter. We can have multiple operators all strung together!
% Scaling [ 4 0 ; 0 4 ]  x  rotation[ sqrt(2) ]  x  flip[ -1 0 ; 0 1 ]
% associativity means we can compute a combined operator all in advance.
