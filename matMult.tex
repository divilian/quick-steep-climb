
\chapter{Matrix multiplication}

So far, we've multiplied scalars by vectors
(p.~\pageref{scalarVectorMultiplication}), vectors by other vectors
(p.~\pageref{dotProduct}), scalars by matrices
(p.~\pageref{scalarMatrixMultiplication}), and even matrices by vectors
(p.~\pageref{matrixVectorMultiplication}). The only thing we haven't done yet
is multiply one entire matrix by another. That mysterious operation is the
subject of this chapter.

Luckily, we've already set ourselves up for success. As it will turn out,
matrix-matrix multiplication is really just matrix-\textit{vector}
multiplication ``in a loop''; \textit{i.e.}, repeated several times.

\section{When it's legal and what you get}

But let's not get ahead of ourselves. First, let's outline the very curious
rules for (1) when two matrices \textit{can} be multiplied at all (often they
can't), and (2) if they can, what the dimensions of the result are. These rules
will surprise you at first (they certainly did me).

Let's say we have two matrices called $A$ and $B$. Suppose that $A$ is an
$m\times n$ matrix ($m$ rows and $n$ columns), and that $B$ is a $p\times q$
matrix. Visually, here's what we've got:

\makeatletter
\newcommand\makebig[2]{%
  \@xp\newcommand\@xp*\csname#1\endcsname{\bBigg@{#2}}%
  \@xp\newcommand\@xp*\csname#1l\endcsname{\@xp\mathopen\csname#1\endcsname}%
  \@xp\newcommand\@xp*\csname#1r\endcsname{\@xp\mathclose\csname#1\endcsname}%
}
\makeatother

\makebig{Biggg} {3.5}
\makebig{Bigggg} {4.0}
\makebig{Biggggg} {4.5}

\vspace{-.15in}
\begin{align*}
m \Biggg\{
\overbrace{
\begin{bmatrix}
\smallblacksquare & \smallblacksquare & \cdots & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \cdots & \smallblacksquare \\
\vdots & \vdots & \cdots & \vdots \\
\smallblacksquare & \smallblacksquare & \cdots & \smallblacksquare \\
\end{bmatrix}}^n \ \  \smallblackcircle \ \ 
p \Biggg\{
\overbrace{
\begin{bmatrix}
\smallblacksquare & \smallblacksquare & \cdots & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \cdots & \smallblacksquare \\
\vdots & \vdots & \cdots & \vdots \\
\smallblacksquare & \smallblacksquare & \cdots & \smallblacksquare \\
\end{bmatrix}}^q
\ \ = \ \ \text{\LARGE ?}
\end{align*}
\vspace{-.15in}

\smallskip
Here are the rules:

\begin{center}
\begin{framed}
\begin{compactenum}
\item $n$ must be equal to $p$, or you can't multiply the matrices at all.
\item If $n$ does equal $p$, then you'll get an $m\times q$ matrix when you
multiply them.
\end{compactenum}
\end{framed}
\end{center}

Those rules are so strange and unexpected that it's worth taking a long moment
to stare at both the matrices and the rules and try to digest them.

\smallskip

Some concrete examples:

\begin{enumerate}
\itemsep.5em

\item Can we multiply a $3\times 2$ matrix by a $2\times 4$? Yes, since $n=2$
and $p=2$. And our result will be a $3\times 4$:

\vspace{-.15in}
\begin{align*}
3 \Biggg\{
\overbrace{
\begin{bmatrix}
\smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare \\
\end{bmatrix}}^2 \ \  \cdot \ \ 
2 \Bigg\{
\overbrace{
\begin{bmatrix}
\smallblacksquare & \smallblacksquare & \smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \smallblacksquare & \smallblacksquare \\
\end{bmatrix}}^4
\ \ = \ \ 
3 \Biggg\{
\overbrace{
\begin{bmatrix}
\smallblacksquare & \smallblacksquare & \smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \smallblacksquare & \smallblacksquare \\
\end{bmatrix}}^4.
\end{align*}
\vspace{-.15in}

\item Can we multiply a $2\times 5$ matrix by a $5\times 3$? Yes, since $n=5$
and $p=5$. And we get a $2\times 3$:

\vspace{-.15in}
\begin{align*}
2 \Bigg\{
\overbrace{
\begin{bmatrix}
\smallblacksquare & \smallblacksquare& \smallblacksquare& \smallblacksquare& \smallblacksquare \\
\smallblacksquare & \smallblacksquare& \smallblacksquare& \smallblacksquare& \smallblacksquare \\
\end{bmatrix}}^5 \ \  \cdot \ \ 
5 \Biggggg\{
\overbrace{
\begin{bmatrix}
\smallblacksquare & \smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \smallblacksquare \\
\end{bmatrix}}^3
\ \ = \ \ 
2 \Bigg\{
\overbrace{
\begin{bmatrix}
\smallblacksquare & \smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \smallblacksquare \\
\end{bmatrix}}^3.
\end{align*}
\vspace{-.15in}

\item Can we multiply a $4\times 3$ matrix by another $4\times 3$? No, since
$n=3$ but $p=4$. Sorry.

\vspace{-.15in}
\begin{align*}
4 \Bigggg\{
\overbrace{
\begin{bmatrix}
\smallblacksquare & \smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \smallblacksquare \\
\end{bmatrix}}^3 \ \  \smallblackcircle \ \ 
4 \Bigggg\{
\overbrace{
\begin{bmatrix}
\smallblacksquare & \smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \smallblacksquare \\
\smallblacksquare & \smallblacksquare & \smallblacksquare \\
\end{bmatrix}}^3
\ \ = \ \ \text{NOPE}.
\end{align*}
\vspace{-.15in}
\end{enumerate}

It's sooo bizarre. Sometimes you multiply two biggish matrices together and get
a small one; sometimes you multiply narrow ones and get a tall one; sometimes
it seems like you'd get a valid answer and yet there is none.

\section{Lather, rinse, repeat}

Now that we have the ground rules for what the resulting matrix will be shaped
like (if there even is one) let's talk about actually calculating the entries.
There are several ways to think about this, each of which sheds a different
light on the operation.

\index{matrix-vector multiplication}

The first way is to view the matrix multiplication $A \cdot B$ as
\textbf{repeated matrix-vector multiplication}, where the matrix is $A$ and the
vectors are the \textbf{columns} of $B$. The final answer is formed by
stitching together the results of the individual matrix-vector multiplications.

Let's see it in action. If you remember the procedure on
p.~\pageref{matrixVectorMultiplication}, you can confirm that if we perform
this matrix-vector multiplication:

\vspace{-.15in}
\begin{align*}
\begin{bmatrix}
2 & 1 & 5 \\
0 & 3 & -2 \\
\end{bmatrix} \cdot
\begin{bmatrix}
0 \\ 0 \\ 7 \\
\end{bmatrix},
\end{align*}
\vspace{-.15in}

we'll get the answer

\vspace{-.15in}
\begin{align*}
\begin{bmatrix}
35 \\ -14 \\
\end{bmatrix}.
\end{align*}
\vspace{-.15in}

\smallskip
And if we do this:

\vspace{-.15in}
\begin{align*}
\begin{bmatrix}
2 & 1 & 5 \\
0 & 3 & -2 \\
\end{bmatrix} \cdot
\begin{bmatrix}
9 \\ 99 \\ 999 \\
\end{bmatrix},
\end{align*}
\vspace{-.15in}

we'll get this:

\vspace{-.15in}
\begin{align*}
\begin{bmatrix}
5112 \\ -1701 \\
\end{bmatrix}.
\end{align*}
\vspace{-.15in}

\smallskip
Finally, if we do this:

\vspace{-.15in}
\begin{align*}
\begin{bmatrix}
2 & 1 & 5 \\
0 & 3 & -2 \\
\end{bmatrix} \cdot
\begin{bmatrix}
-13 \\ -13 \\ -13 \\
\end{bmatrix},
\end{align*}
\vspace{-.15in}

we'll get this:

\vspace{-.15in}
\begin{align*}
\begin{bmatrix}
-104 \\ -13 \\
\end{bmatrix}.
\end{align*}
\vspace{-.15in}

\smallskip

Notice what I did there. I took the \textit{same} $2\times 3$ matrix each time,
and multiplied it by some vector -- a weird one, to help jog your memory in a
moment -- to get an answer.

All right. Now let's see what happens if I perform the following
matrix-\textit{matrix} multiplication:

\vspace{-.15in}
\begin{align*}
\begin{bmatrix}
2 & 1 & 5 \\
0 & 3 & -2 \\
\end{bmatrix} \cdot
\begin{bmatrix}
0 & 9 & -13 \\
0 & 99 & -13 \\
7 & 999 & -13 \\
\end{bmatrix} \ = \ {\LARGE ?}
\end{align*}
\vspace{-.15in}

Examine the columns of the right-hand matrix: they should ring a bell. Each
\textit{column} is one of the \textit{vectors} that I just multiplied our
matrix by to get a columnar answer. The result of this operation is achieved by
simply putting all those columnar answers together:

\vspace{-.15in}
\begin{align*}
\begin{bmatrix}
2 & 1 & 5 \\
0 & 3 & -2 \\
\end{bmatrix} \cdot
\begin{bmatrix}
0 & 9 & -13 \\
0 & 99 & -13 \\
7 & 999 & -13 \\
\end{bmatrix} =
\begin{bmatrix}
35 & 5112 & -104 \\
-14 & -1701 & -13 \\
\end{bmatrix}.
\end{align*}
\vspace{-.15in}

\index{Bond, James}

See how that works? The result of the multiplication is just the three
individual matrix-vector products, all concatenated together in an ``answer
matrix.'' The left column of our answer is ${\scriptsize \begin{bmatrix} 35 \\
-14 \\ \end{bmatrix}}$, which is exactly what we got when we multiplied that
left-hand matrix by James Bond. The right column of our answer is ${\scriptsize
\begin{bmatrix} -104 \\ -13 \\ \end{bmatrix}}$, which is what we got when we
multiplied the matrix by triple $-13$'s. And the middle column of the answer is
the matrix times the stack of nines. So you can see that matrix-\textit{matrix}
multiplication is really just repeated matrix-\textit{vector} multiplication.

This way of thinking about matrix multiplication might be the one that
resonates most strongly with you. (It did for me.)

%\subsection{Outer and inner products}
%
%Because we can treat a vector as a sort of degenerate matrix (with only one
%row, or only one column), it sometimes makes sense to do this matrix-vector
%multiplication with two vectors. Which one is treated as a row vector and which
%one is treated as a column vector makes all the difference.
%
%As an illustration, I'm going to define vectors $\overrightarrow{\textbf{p}}$
%and $\overrightarrow{\textbf{q}}$ this way:
%
%\vspace{-.15in}
%\begin{align*}
%\overrightarrow{\textbf{p}} =
%\begin{bmatrix}
%3 & 1 & 2 \\
%\end{bmatrix}, \quad 
%\overrightarrow{\textbf{q}} =
%\begin{bmatrix}
%5 \\  4 \\ -3 \\
%\end{bmatrix}.
%\end{align*}
%\vspace{-.15in}
%
%So $\overrightarrow{\textbf{p}}$ is a row vector, and
%$\overrightarrow{\textbf{q}}$ is a column vector.
%
%Now, treating $\overrightarrow{\textbf{p}}$ as a $1\times 3$ matrix, we perform
%multiplication and get:
%
%\vspace{-.15in}
%\begin{align*}
%\overrightarrow{\textbf{p}} \cdot \overrightarrow{\textbf{q}} =
%\begin{bmatrix}
%3 & 1 & 2 \\
%\end{bmatrix} \cdot
%\begin{bmatrix}
%5 \\ 4 \\ -3 \\
%\end{bmatrix} = 13.
%\end{align*}
%\vspace{-.15in}
%
%\index{inner product}
%It's just the dot product, of course, calculated in the usual way. Here, for
%reasons that will shortly become clear, we also call it the \textbf{inner
%product} of the two vectors.
%
%Now suppose I swap the order, and compute $\overrightarrow{\textbf{q}}$ times
%$\overrightarrow{\textbf{p}}$ instead. What would I get? The answer will surely
%surprise you:
%
%\vspace{-.15in}
%\begin{align*}
%\overrightarrow{\textbf{q}} \cdot \overrightarrow{\textbf{p}} =
%\begin{bmatrix}
%5 \\ 4 \\ -3 \\
%\end{bmatrix} \cdot
%\begin{bmatrix}
%3 & 1 & 2 \\
%\end{bmatrix} =
%\begin{bmatrix}
%15 & 5 & 10 \\
%12 & 4 & 8 \\
%-9 & -3 & -6 \\
%\end{bmatrix}.
%\end{align*}
%\vspace{-.15in}
%
%Hooooooo...\textit{what?!} $\overrightarrow{\textbf{p}}$ times
%$\overrightarrow{\textbf{q}}$ is the number 13, but
%$\overrightarrow{\textbf{q}}$ times $\overrightarrow{\textbf{p}}$ is an entire
%grid full of numbers?
%
%Yes it is. Here's why. 




% Recall from linear ops chapter. We can have multiple operators all strung together!
% Scaling [ 4 0 ; 0 4 ]  x  rotation[ sqrt(2) ]  x  flip[ -1 0 ; 0 1 ]
% associativity means we can compute a combined operator all in advance.
